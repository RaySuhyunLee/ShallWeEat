FANN_FLO_2.1
num_layers=6
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=4 4 5 4 5 11 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (4, 4, 5.00000000000000000000e-01) (4, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (4, 4, 5.00000000000000000000e-01) (4, 4, 5.00000000000000000000e-01) (4, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (4, 4, 5.00000000000000000000e-01) (4, 4, 5.00000000000000000000e-01) (4, 4, 5.00000000000000000000e-01) (4, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (0, 0, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 1.20630413293838500977e-02) (1, -5.65527193248271942139e-02) (2, -8.15752670168876647949e-02) (3, -3.54985222220420837402e-02) (0, -2.36452668905258178711e-02) (1, -5.93585520982742309570e-03) (2, 3.60806658864021301270e-02) (3, 7.64147937297821044922e-03) (0, 3.04104164242744445801e-02) (1, -9.23132747411727905273e-02) (2, 9.08102616667747497559e-02) (3, 4.81835976243019104004e-02) (4, 2.16278210282325744629e-02) (5, 9.87560972571372985840e-02) (6, -6.18262588977813720703e-03) (7, 8.86527970433235168457e-02) (4, -1.24292224645614624023e-02) (5, -9.79804098606109619141e-02) (6, 4.32765409350395202637e-02) (7, -5.13115637004375457764e-02) (4, 6.57274574041366577148e-03) (5, 6.81572780013084411621e-02) (6, -8.06539282202720642090e-02) (7, 4.94723841547966003418e-02) (4, 8.23068693280220031738e-02) (5, -6.83534294366836547852e-02) (6, -1.60303786396980285645e-02) (7, -2.25887969136238098145e-02) (8, -4.98433820903301239014e-02) (9, 8.23136046528816223145e-02) (10, 4.47031781077384948730e-02) (11, -7.37579762935638427734e-02) (12, -5.03464937210083007812e-02) (8, 2.65023335814476013184e-02) (9, 2.47183665633201599121e-02) (10, 4.15251925587654113770e-02) (11, -8.60499888658523559570e-02) (12, -4.21925000846385955811e-02) (8, 7.06624910235404968262e-02) (9, 2.44159698486328125000e-02) (10, -4.08642180263996124268e-02) (11, -4.90564852952957153320e-03) (12, -4.91886995732784271240e-02) (13, 8.55423286557197570801e-02) (14, -9.01888385415077209473e-02) (15, -3.75137478113174438477e-03) (16, -4.93630170822143554688e-02) (13, -4.42699678242206573486e-02) (14, -4.53331358730792999268e-02) (15, 8.60374644398689270020e-02) (16, 3.16412523388862609863e-02) (13, -5.46934455633163452148e-03) (14, 7.67220929265022277832e-02) (15, 6.80951103568077087402e-02) (16, 7.44710043072700500488e-02) (13, 3.39907482266426086426e-02) (14, 8.25565829873085021973e-02) (15, -7.15161263942718505859e-02) (16, 2.84044370055198669434e-02) (17, -6.67789578437805175781e-03) (18, -3.54203283786773681641e-02) (19, 9.05586555600166320801e-02) (20, 1.94491520524024963379e-02) (21, 8.17836597561836242676e-02) (17, -6.22059330344200134277e-02) (18, -9.50833410024642944336e-02) (19, -6.57450631260871887207e-02) (20, 2.27650031447410583496e-02) (21, 1.14401206374168395996e-02) (17, 7.40959122776985168457e-02) (18, -7.01575130224227905273e-02) (19, 6.26694783568382263184e-02) (20, 8.58504548668861389160e-02) (21, 8.84899571537971496582e-02) (17, 5.08193597197532653809e-02) (18, -7.89667889475822448730e-02) (19, 5.15327602624893188477e-03) (20, 1.11469998955726623535e-02) (21, -5.24141490459442138672e-02) (17, 7.54208341240882873535e-02) (18, -2.00350582599639892578e-03) (19, -7.28621706366539001465e-02) (20, 5.50614297389984130859e-03) (21, -5.81618621945381164551e-02) (17, 7.36059024930000305176e-02) (18, 9.44108292460441589355e-02) (19, -3.73541265726089477539e-02) (20, -1.08408182859420776367e-02) (21, -1.60875171422958374023e-03) (17, -3.83440740406513214111e-02) (18, -4.88785281777381896973e-02) (19, 9.85741838812828063965e-02) (20, -6.36508241295814514160e-02) (21, 2.06460952758789062500e-02) (17, -1.05071067810058593750e-03) (18, -5.92779405415058135986e-02) (19, -8.43322724103927612305e-02) (20, 2.74554565548896789551e-02) (21, 4.38953116536140441895e-02) (17, -5.14098219573497772217e-02) (18, -4.48694303631782531738e-02) (19, 7.94811919331550598145e-02) (20, 4.03097048401832580566e-02) (21, 8.51352140307426452637e-02) (17, 6.75405338406562805176e-02) (18, -4.62605170905590057373e-02) (19, 9.95022132992744445801e-02) (20, -6.64951801300048828125e-02) (21, 1.55610293149948120117e-02) 
