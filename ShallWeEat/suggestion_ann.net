FANN_FLO_2.1
num_layers=2
learning_rate=0.700000
connection_rate=1.000000
network_type=1
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=5 10 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) (5, 4, 5.00000000000000000000e-01) 
connections (connected_to_neuron, weight)=(0, -2.82817018403434250473e+00) (1, -2.95837050302648041367e+00) (2, -2.93229619412445519089e+00) (3, -2.79029372705840561508e+00) (4, -1.10610700549266471882e+00) (0, -2.85193463041209671616e+00) (1, -2.94596358760022614121e+00) (2, -2.92734070866131279587e+00) (3, -2.86049385740184280991e+00) (4, -1.09192753733775749225e+00) (0, -2.44595639436268275091e+00) (1, -2.41517244680547182867e+00) (2, -2.45291531114601557562e+00) (3, -2.35174625768088763067e+00) (4, -6.02923380152202925686e-01) (0, -2.32250456234431634783e+00) (1, -2.21527450254178415179e+00) (2, -2.31244701987123857378e+00) (3, -2.27894647141909967303e+00) (4, -4.46424382412181408597e-01) (0, -1.95835660507893916105e+00) (1, -1.92727336397385951017e+00) (2, -1.94235229602074976896e+00) (3, -1.96169939747071619962e+00) (4, -1.79140473477266670699e+00) (0, -2.31050898974752794146e+00) (1, -2.21252309253430734515e+00) (2, -2.35006241967893014788e+00) (3, -2.22924051156258951067e+00) (4, -5.56291359150157371793e-01) (0, -2.00133778264737482999e+00) (1, -2.04452791919923182462e+00) (2, -2.08776334783053751920e+00) (3, -1.96075120678162928556e+00) (4, -5.15954627328858772550e-01) (0, -4.27272348890713749014e-01) (1, -4.60103570709637699210e-01) (2, -4.55320201108864841544e-01) (3, -3.81730287561825809561e-01) (4, 2.02747557381343934679e+00) (0, -2.18004875352597604632e+00) (1, -2.35450345745301614642e+00) (2, -2.28970388790583978533e+00) (3, -2.34008132388806711077e+00) (4, -4.27382695162044079495e-01) (0, -1.19811134399718777210e+00) (1, -1.31122660251206890614e+00) (2, -1.27728218944615856678e+00) (3, -1.34681652845687405140e+00) (4, -8.78976509561293140038e-01) 
